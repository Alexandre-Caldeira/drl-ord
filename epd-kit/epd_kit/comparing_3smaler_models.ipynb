{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([5, 2.5, 0, -2.5, -5])\n",
      "(2500, 18, 7, 120)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gymnasium import spaces\n",
    "import torch as th\n",
    "\n",
    "with open('c_states_dict_v4.pkl', 'rb') as f:\n",
    "    c_states = pickle.load(f)\n",
    "\n",
    "def my_norm(vec):\n",
    "    vec = vec-np.mean(vec)\n",
    "    vec = vec/np.std(vec)\n",
    "    return vec\n",
    "\n",
    "# for snr in c_states.keys():\n",
    "#     data = c_states[snr]\n",
    "#     for episode in range(data.shape[0]):\n",
    "        \n",
    "print(c_states.keys())\n",
    "print(c_states[5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FrequencyDetectionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Frequency Objective Response Detection that follows gymnasium interface\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(FrequencyDetectionEnv, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.episodes = data.shape[0]-1\n",
    "        self.frequencies = data.shape[1]-1\n",
    "        self.measures = data.shape[2]\n",
    "        self.windows = data.shape[3]-1\n",
    "        self.current_episode = 0\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "\n",
    "        # Action space: 0 (no detection) or 1 (detection)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation space: 5 measures for each of the 4 frequencies (1 focus + 3 noise)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4*self.measures,), dtype=np.float32)\n",
    "\n",
    "        # Initialize the internal state\n",
    "        self.state = None\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0   \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        self.latest_result = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to an initial state and returns the initial observation.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if  self.current_episode< self.episodes:\n",
    "            self.current_episode +=1\n",
    "        else:\n",
    "            self.current_episode = np.random.randint(0, self.episodes)\n",
    "\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0        \n",
    "         \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        \n",
    "        # Initialize the state\n",
    "        self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "        return self.state, self.latest_result\n",
    "\n",
    "    def _get_state(self, frequency, window, episode= 0):\n",
    "        \"\"\"Helper function to get the state at a given frequency and window.\"\"\"\n",
    "         \n",
    "        if episode != 0:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        focus_measure = self.data[self.current_episode, frequency, :, window]\n",
    "        \n",
    "        # Select 3 random noise frequencies, excluding the current focus frequency\n",
    "        noise_indices = np.setdiff1d(np.arange(9, 17), frequency - 9 if frequency >= 9 else [])\n",
    "        noise_frequencies = np.random.choice(noise_indices, 3, replace=False)\n",
    "        noise_measures = [self.data[self.current_episode, nf, :, window] for nf in noise_frequencies]\n",
    "         \n",
    "        measures = np.concatenate(([focus_measure], noise_measures), axis=0).flatten()\n",
    "\n",
    "        return np.array(measures, dtype=np.float32)\n",
    "\n",
    "    def step(self, action, episode = -1):\n",
    "        \"\"\"Executes one time step within the environment.\"\"\"\n",
    "        # Returns:\n",
    "        # tuple (observation, reward, terminated, truncated, info).\n",
    "        # Return type:\n",
    "        # Tuple[Tuple | Dict[str, Any] | ndarray | int, float, bool, bool, Dict]\n",
    "        \n",
    "        if episode != -1:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        reward = 0\n",
    "        kp_fp = 1\n",
    "        fp_des = 0.05\n",
    "        \n",
    "        should_detect = self.current_frequency <= 8\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                self.true_positives += 1\n",
    "            else:\n",
    "                self.false_positives += 1\n",
    "\n",
    "        self.current_frequency += 1\n",
    "        window_done = self.current_frequency >= self.frequencies\n",
    "        epsiode_terminated = self.current_window >= self.windows\n",
    "        info = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "\n",
    "        if window_done:\n",
    "            self.state = self._get_state(self.current_frequency-1, self.current_window)\n",
    "\n",
    "            current_ep_prog = 100\n",
    "            \n",
    "            self.tp_rate = 100*self.true_positives / (self.frequencies//2+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.frequencies//2+1)\n",
    "                \n",
    "            reward = +( -(((100-fp_des)**2)/100 +(fp_des**2)/-100) +((self.fp_rate)**2)/(-100)+ ((self.tp_rate)**2)/(100) )*current_ep_prog/100\n",
    "            \n",
    "            self.tpr_hist.append(self.tp_rate)\n",
    "            self.fpr_hist.append(self.fp_rate)\n",
    "\n",
    "            if not(epsiode_terminated):\n",
    "                self.current_window += 1\n",
    "                self.current_frequency = 0\n",
    "                self.true_positives = 0\n",
    "                self.false_positives = 0\n",
    "                    \n",
    "        if epsiode_terminated:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "            current_ep_prog =100\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.current_frequency+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.current_frequency+1)\n",
    "\n",
    "            reward = +( -(((100-fp_des)**2)/100+(fp_des**2)/-100)\n",
    "                    +(kp_fp*(np.round(np.mean(self.fpr_hist),4))**2)/(-100)\n",
    "                    +((np.round(np.mean(self.tpr_hist),4))**2)/(100))*current_ep_prog/100\n",
    "           \n",
    "            truncated = False\n",
    "\n",
    "            info = {'tp_rate':np.round(np.mean(self.tpr_hist),2),'fp_rate':np.round(np.mean(self.fpr_hist),2)}\n",
    "            self.latest_result = info\n",
    "\n",
    "            self.tpr_hist = []\n",
    "            self.fpr_hist = [] \n",
    "            self.current_window = 2\n",
    "            self.current_frequency = 0\n",
    "            self.true_positives = 0\n",
    "            self.false_positives = 0\n",
    "\n",
    "        else:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 1\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 1\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-kp_fp*fp-fn+tn\n",
    "\n",
    "            truncated = False\n",
    "\n",
    "        return self.state, reward, epsiode_terminated, truncated, info\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment for visualization purposes.\"\"\"\n",
    "        if mode=='human':\n",
    "            print(f'Window: {self.current_window}, Frequency: {self.current_frequency},TP: {self.tp_rate}, FP: {self.fp_rate}')\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up any resources used by the environment.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST_FrequencyDetectionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Frequency Objective Response Detection that follows gymnasium interface\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(TEST_FrequencyDetectionEnv, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.episodes = data.shape[0]-1\n",
    "        self.frequencies = data.shape[1]-1\n",
    "        self.measures = data.shape[2]\n",
    "        self.windows = data.shape[3]-1\n",
    "        self.current_episode = 0\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "\n",
    "        # Action space: 0 (no detection) or 1 (detection)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation space: 5 measures for each of the 4 frequencies (1 focus + 3 noise)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4*self.measures,), dtype=np.float32)\n",
    "\n",
    "        # Initialize the internal state\n",
    "        self.state = None\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0   \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []  \n",
    "        self.latest_result = {'tp_rate':np.nan,'fp_rate':np.nan}   \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to an initial state and returns the initial observation.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if  self.current_episode< self.episodes:\n",
    "            self.current_episode +=1\n",
    "        else:\n",
    "            self.current_episode = np.random.randint(0, self.episodes)\n",
    "\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0        \n",
    "         \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        \n",
    "        # Initialize the state\n",
    "        self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "        return self.state, self.latest_result\n",
    "\n",
    "    def _get_state(self, frequency, window, episode= 0):\n",
    "        \"\"\"Helper function to get the state at a given frequency and window.\"\"\"\n",
    "         \n",
    "        if episode != 0:\n",
    "            self.current_episode = episode\n",
    "         \n",
    "        # print(frequency)\n",
    "        focus_measure = self.data[self.current_episode, frequency, :, window]\n",
    "        \n",
    "        # Select 3 random noise frequencies, excluding the current focus frequency\n",
    "        noise_indices = np.setdiff1d(np.arange(9, 17), frequency - 9 if frequency >= 9 else [])\n",
    "        noise_frequencies = np.random.choice(noise_indices, 3, replace=False)\n",
    "        noise_measures = [self.data[self.current_episode, nf, :, window] for nf in noise_frequencies]\n",
    "         \n",
    "        measures = np.concatenate(([focus_measure], noise_measures), axis=0).flatten()\n",
    "       \n",
    "        return np.array(measures, dtype=np.float32)\n",
    "\n",
    "    def step(self, action, episode = -1):\n",
    "        \"\"\"Executes one time step within the environment.\"\"\"\n",
    "        # Returns:\n",
    "        # tuple (observation, reward, terminated, truncated, info).\n",
    "        # Return type:\n",
    "        # Tuple[Tuple | Dict[str, Any] | ndarray | int, float, bool, bool, Dict]\n",
    "\n",
    "        if episode != -1:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        reward = 0\n",
    "        fp_des = 0.05\n",
    "\n",
    "        should_detect = self.current_frequency <= 8\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                self.true_positives += 1\n",
    "            else:\n",
    "                self.false_positives += 1\n",
    "\n",
    "        self.current_frequency += 1\n",
    "        window_done = self.current_frequency >= self.frequencies\n",
    "        epsiode_terminated = self.current_window >= self.windows\n",
    "        info = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "        if window_done:\n",
    "            self.state = self._get_state(self.current_frequency-1, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.frequencies//2+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.frequencies//2+1)\n",
    "\n",
    "            self.tpr_hist.append(self.tp_rate)\n",
    "            self.fpr_hist.append(self.fp_rate)\n",
    "\n",
    "            if not(epsiode_terminated):\n",
    "                self.current_window += 1\n",
    "                self.current_frequency = 0\n",
    "                self.true_positives = 0\n",
    "                self.false_positives = 0\n",
    "                    \n",
    "        if epsiode_terminated:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.current_frequency+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.current_frequency+1)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "           \n",
    "            truncated = False\n",
    "\n",
    "            info = {'tp_rate':np.round(np.mean(self.tpr_hist),2),'fp_rate':np.round(np.mean(self.fpr_hist),2)}\n",
    "            self.latest_result = info\n",
    "\n",
    "            self.tpr_hist = []\n",
    "            self.fpr_hist = [] \n",
    "            self.current_window = 2\n",
    "            self.current_frequency = 0\n",
    "            self.true_positives = 0\n",
    "            self.false_positives = 0\n",
    "\n",
    "        else:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "            truncated = False\n",
    "\n",
    "        reward = tp-fp-fn+tn\n",
    "\n",
    "        return self.state, reward, epsiode_terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment for visualization purposes.\"\"\"\n",
    "        if mode =='human':\n",
    "            print(f'Window: {self.current_window}, Frequency: {self.current_frequency},TP: {self.tp_rate}, FP: {self.fp_rate}')\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up any resources used by the environment.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total decisions: 200600\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "snr = 0\n",
    "data = c_states[snr]\n",
    "timesteps = (120-2)*(18-1)*100\n",
    "print(f'Total decisions: {timesteps}')\n",
    "print(data.shape[2]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.99e+03 |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 1005     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | -64         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010760534 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | -0.29       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.74        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    value_loss           | 47.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | -29.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006543142 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.029       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.15        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    value_loss           | 46.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 31.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011482291 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.0873      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00925    |\n",
      "|    value_loss           | 42.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 71.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 720         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008690888 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.02        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    value_loss           | 35          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 110         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 724         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009095111 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.0919      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.3        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    value_loss           | 42.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 200          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 715          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052631213 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00318     |\n",
      "|    value_loss           | 39.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 283          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 708          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077592363 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.421       |\n",
      "|    explained_variance   | -0.0476      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.5         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0051      |\n",
      "|    value_loss           | 32.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 331          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 707          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049016755 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | -0.215       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.09         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00225     |\n",
      "|    value_loss           | 29.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 370         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006573175 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | -0.0877     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38          |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    value_loss           | 31          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 363          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 705          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033960498 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | -0.0489      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.06         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 394           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 707           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076077855 |\n",
      "|    clip_fraction        | 0.0262        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.342        |\n",
      "|    explained_variance   | -0.255        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 108           |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000332     |\n",
      "|    value_loss           | 58            |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 454          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 711          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025209263 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | -0.0482      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.6         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 42.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 483          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 714          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019888585 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | -0.117       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.15         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 26.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 544          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 717          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004065501 |\n",
      "|    clip_fraction        | 0.00967      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.158       |\n",
      "|    explained_variance   | -0.147       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.4         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000494    |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 550         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 717         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004589773 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.176      |\n",
      "|    explained_variance   | -0.21       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00229    |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 554          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 718          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028743958 |\n",
      "|    clip_fraction        | 0.0439       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | -0.0863      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 57.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 597          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 719          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056681945 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | -0.226       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.4         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00474     |\n",
      "|    value_loss           | 69.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 626          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 709          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010582505 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.152       |\n",
      "|    explained_variance   | -0.0387      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 32.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 649          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 703          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030513862 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    value_loss           | 37.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 660          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 699          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013407716 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | -0.324       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000123    |\n",
      "|    value_loss           | 48.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 653         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 697         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001935494 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | -0.0675     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000473   |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 683          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 696          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016766565 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | -0.0559      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.8         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 73.7         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 709           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 685           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 71            |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086105376 |\n",
      "|    clip_fraction        | 0.0122        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.113        |\n",
      "|    explained_variance   | -0.321        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 11.1          |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000308     |\n",
      "|    value_loss           | 32.8          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 730           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 678           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 75            |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069408107 |\n",
      "|    clip_fraction        | 0.0129        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.121        |\n",
      "|    explained_variance   | -0.293        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 27.4          |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.000684     |\n",
      "|    value_loss           | 37.4          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 745          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026128981 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.0247       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.9         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | 0.000219     |\n",
      "|    value_loss           | 38.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 756          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 675          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018140678 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.193       |\n",
      "|    explained_variance   | 0.0494       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.35         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.000725     |\n",
      "|    value_loss           | 46.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 766          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022919308 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | -0.0912      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.5         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000513    |\n",
      "|    value_loss           | 57.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 778         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 672         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003874568 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.176      |\n",
      "|    explained_variance   | -0.0991     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 67.1        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00138    |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 795         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 673         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006862481 |\n",
      "|    clip_fraction        | 0.0233      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | -0.143      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 75.6        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0006     |\n",
      "|    value_loss           | 50.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 802          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018180714 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.157       |\n",
      "|    explained_variance   | 0.145        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.7         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | 0.000219     |\n",
      "|    value_loss           | 42.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 820          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 674          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032418177 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0867      |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 123          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000784    |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 825          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 676          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008784823 |\n",
      "|    clip_fraction        | 0.00869      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0962      |\n",
      "|    explained_variance   | 0.154        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.3         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000279    |\n",
      "|    value_loss           | 32.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 838          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013955515 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | -0.227       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 59.7         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 851           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 677           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 105           |\n",
      "|    total_timesteps      | 71680         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038283656 |\n",
      "|    clip_fraction        | 0.0188        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.131        |\n",
      "|    explained_variance   | -0.157        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 36.9          |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | 0.0011        |\n",
      "|    value_loss           | 45            |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 867           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 677           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 108           |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00080779043 |\n",
      "|    clip_fraction        | 0.0201        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.192        |\n",
      "|    explained_variance   | -0.12         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 53.7          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | 0.000448      |\n",
      "|    value_loss           | 98.3          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 881           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 676           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 111           |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00063694373 |\n",
      "|    clip_fraction        | 0.00649       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0494       |\n",
      "|    explained_variance   | -0.661        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 25.4          |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.000879     |\n",
      "|    value_loss           | 43.6          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 896           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 677           |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 114           |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00067972223 |\n",
      "|    clip_fraction        | 0.00566       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0622       |\n",
      "|    explained_variance   | -0.545        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 17.9          |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -3.84e-05     |\n",
      "|    value_loss           | 46.2          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 903           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 676           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 118           |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00077595754 |\n",
      "|    clip_fraction        | 0.00879       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0644       |\n",
      "|    explained_variance   | -0.295        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 35.9          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00101      |\n",
      "|    value_loss           | 36.3          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 905          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024871866 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.163        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42           |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | 0.000566     |\n",
      "|    value_loss           | 60.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 913         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 678         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002521298 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | -0.277      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.6        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.000427   |\n",
      "|    value_loss           | 60          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 913          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 679          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016550105 |\n",
      "|    clip_fraction        | 0.00903      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | -0.204       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.7         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000622    |\n",
      "|    value_loss           | 47.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 914          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 676          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 130          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015945442 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0806      |\n",
      "|    explained_variance   | 0.0852       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000421    |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 905          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 133          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012647433 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | -0.165       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.1         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.000945     |\n",
      "|    value_loss           | 59.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 913          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 674          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009982248 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.0878       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000872    |\n",
      "|    value_loss           | 84.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 916          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032640419 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.114       |\n",
      "|    explained_variance   | 0.164        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 34.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 925         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 674         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008023809 |\n",
      "|    clip_fraction        | 0.0394      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.0898      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.000612   |\n",
      "|    value_loss           | 53.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 930          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 676          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008629495 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.117       |\n",
      "|    explained_variance   | -0.206       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.39         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000672    |\n",
      "|    value_loss           | 43.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 940          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 677          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 148          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037025982 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0988      |\n",
      "|    explained_variance   | -0.298       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.8          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000649    |\n",
      "|    value_loss           | 55.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 945          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 678          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 150          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005691558 |\n",
      "|    clip_fraction        | 0.00581      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0553      |\n",
      "|    explained_variance   | 0.299        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.8         |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 34.9         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 953           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 678           |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 153           |\n",
      "|    total_timesteps      | 104448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00061459036 |\n",
      "|    clip_fraction        | 0.0252        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.111        |\n",
      "|    explained_variance   | -0.0688       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.61          |\n",
      "|    n_updates            | 500           |\n",
      "|    policy_gradient_loss | 0.00085       |\n",
      "|    value_loss           | 43            |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 962         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 679         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001255123 |\n",
      "|    clip_fraction        | 0.0127      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0678     |\n",
      "|    explained_variance   | -0.124      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 8.16e-06    |\n",
      "|    value_loss           | 44.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.99e+03     |\n",
      "|    ep_rew_mean          | 970          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 680          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 159          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010471558 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0643      |\n",
      "|    explained_variance   | 0.00588      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 978         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 680         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001909456 |\n",
      "|    clip_fraction        | 0.00884     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0561     |\n",
      "|    explained_variance   | 0.0137      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -1.06e-05   |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.99e+03      |\n",
      "|    ep_rew_mean          | 987           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 679           |\n",
      "|    iterations           | 55            |\n",
      "|    time_elapsed         | 165           |\n",
      "|    total_timesteps      | 112640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064089854 |\n",
      "|    clip_fraction        | 0.00737       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0438       |\n",
      "|    explained_variance   | -0.363        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 10.5          |\n",
      "|    n_updates            | 540           |\n",
      "|    policy_gradient_loss | -0.000486     |\n",
      "|    value_loss           | 37.5          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.99e+03    |\n",
      "|    ep_rew_mean          | 995         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 679         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000983722 |\n",
      "|    clip_fraction        | 0.00566     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0375     |\n",
      "|    explained_variance   | -0.317      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.000481   |\n",
      "|    value_loss           | 35.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m FrequencyDetectionEnv(data)\n\u001b[0;32m      7\u001b[0m model_ppo \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;66;03m#, learning_rate=5*1e-4,gamma=0.9,\u001b[39;00m\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;66;03m#exploration_fraction= 0.1, exploration_initial_eps = 1, exploration_final_eps = 0.05\u001b[39;00m\n\u001b[0;32m      9\u001b[0m             ,policy_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(activation_fn\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU,\n\u001b[0;32m     10\u001b[0m                      net_arch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(pi \u001b[38;5;241m=\u001b[39m[data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], vf\u001b[38;5;241m=\u001b[39m[data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     11\u001b[0m                 )\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel_ppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model_ppo\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_ppo_snr5.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m _, latest \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:654\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[0;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m--> 654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[0;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:697\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, MultiCategoricalDistribution):\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:288\u001b[0m, in \u001b[0;36mCategoricalDistribution.proba_distribution\u001b[1;34m(self, action_logits)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfCategoricalDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfCategoricalDistribution:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\distributions\\categorical.py:64\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train PPO model\n",
    "# policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "#                      net_arch=dict(data.shape[2]+1, vf=[data.shape[2]+1])\n",
    "#                      )\n",
    "# net_arch=dict(pi=[25,25], vf=[25,25]))\n",
    "env = FrequencyDetectionEnv(data)\n",
    "model_ppo = PPO('MlpPolicy', env, verbose=1#, learning_rate=5*1e-4,gamma=0.9,\n",
    "            #exploration_fraction= 0.1, exploration_initial_eps = 1, exploration_final_eps = 0.05\n",
    "            ,policy_kwargs=dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi =[data.shape[2]+1], vf=[data.shape[2]+1]))\n",
    "                )\n",
    "model_ppo.learn(total_timesteps=timesteps)\n",
    "model_ppo.save('mini_ppo_snr5.zip')\n",
    "_, latest = env.reset()\n",
    "print(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | -69.6    |\n",
      "|    exploration_rate | 0.623    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 932      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 7960     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.992    |\n",
      "|    n_updates        | 1964     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration_rate | 0.246    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 910      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 15920    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.716    |\n",
      "|    n_updates        | 3954     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 356      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 898      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 23880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.482    |\n",
      "|    n_updates        | 5944     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 585      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 852      |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 31840    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.554    |\n",
      "|    n_updates        | 7934     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 701      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 848      |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 39800    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.359    |\n",
      "|    n_updates        | 9924     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 785      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 844      |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 47760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.287    |\n",
      "|    n_updates        | 11914    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 856      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 841      |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 55720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 13904    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 919      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 839      |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 63680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.289    |\n",
      "|    n_updates        | 15894    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 958      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 832      |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 71640    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.372    |\n",
      "|    n_updates        | 17884    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 992      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 824      |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 79600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.424    |\n",
      "|    n_updates        | 19874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.02e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 808      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 87560    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 21864    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.05e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 800      |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 95520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.332    |\n",
      "|    n_updates        | 23854    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.08e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 103480   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.233    |\n",
      "|    n_updates        | 25844    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.1e+03  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 796      |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 111440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.461    |\n",
      "|    n_updates        | 27834    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.12e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 119400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.383    |\n",
      "|    n_updates        | 29824    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.12e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 127360   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.427    |\n",
      "|    n_updates        | 31814    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.13e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 790      |\n",
      "|    time_elapsed     | 171      |\n",
      "|    total_timesteps  | 135320   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.471    |\n",
      "|    n_updates        | 33804    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.14e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 181      |\n",
      "|    total_timesteps  | 143280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.224    |\n",
      "|    n_updates        | 35794    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.15e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 790      |\n",
      "|    time_elapsed     | 191      |\n",
      "|    total_timesteps  | 151240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.51     |\n",
      "|    n_updates        | 37784    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.16e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 200      |\n",
      "|    total_timesteps  | 159200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.264    |\n",
      "|    n_updates        | 39774    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.17e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 167160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.267    |\n",
      "|    n_updates        | 41764    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.17e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 784      |\n",
      "|    time_elapsed     | 223      |\n",
      "|    total_timesteps  | 175120   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.618    |\n",
      "|    n_updates        | 43754    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.17e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 781      |\n",
      "|    time_elapsed     | 234      |\n",
      "|    total_timesteps  | 183080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.401    |\n",
      "|    n_updates        | 45744    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.18e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 780      |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 191040   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.405    |\n",
      "|    n_updates        | 47734    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.99e+03 |\n",
      "|    ep_rew_mean      | 1.19e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 782      |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 199000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.437    |\n",
      "|    n_updates        | 49724    |\n",
      "----------------------------------\n",
      "{'tp_rate': 97.82, 'fp_rate': 4.84, 'episode': {'r': 1396.464732, 'l': 1990, 't': 254.366967}, 'TimeLimit.truncated': False, 'terminal_observation': array([ -0.19156176,  -0.19470747,  -0.23399125, 100.        ,\n",
      "         1.6665106 ,  23.37709   ,   0.8580225 ,  -1.6883627 ,\n",
      "        -0.39285073,  -1.6834209 , 100.        ,   5.5194736 ,\n",
      "        33.26269   ,   2.7155964 ,  -1.5072337 ,  -0.29159138,\n",
      "        -1.3506023 , 100.        ,   7.1693516 ,  34.888332  ,\n",
      "         1.2129222 ,  -2.1978085 ,  -0.3370427 ,  -1.9813204 ,\n",
      "       100.        ,  23.835014  ,  53.450874  ,  -2.8215396 ],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Train DQN model\n",
    "env = FrequencyDetectionEnv(data)\n",
    "model_dqn = DQN('MlpPolicy', env, verbose=1#, learning_rate=5*1e-4,gamma=0.9,\n",
    "            #exploration_fraction= 0.1, exploration_initial_eps = 1, exploration_final_eps = 0.05\n",
    "            ,policy_kwargs=dict(activation_fn=th.nn.ReLU,net_arch=[data.shape[2]+1])\n",
    "                )\n",
    "model_dqn.learn(total_timesteps=timesteps)\n",
    "model_dqn.save('mini_dqn_snr0.zip')\n",
    "_, latest = env.reset()\n",
    "print(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 583      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.678   |\n",
      "|    explained_variance | -0.211   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.209   |\n",
      "|    value_loss         | 0.706    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 580      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.666   |\n",
      "|    explained_variance | 0.284    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.72     |\n",
      "|    value_loss         | 1.09     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.263    |\n",
      "|    value_loss         | 1.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 563      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.565   |\n",
      "|    explained_variance | -0.346   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.702    |\n",
      "|    value_loss         | 6.28     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 528      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.624   |\n",
      "|    explained_variance | -0.238   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.618   |\n",
      "|    value_loss         | 1.41     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 528      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.639   |\n",
      "|    explained_variance | 0.143    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -1.76    |\n",
      "|    value_loss         | 3.96     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.99e+03  |\n",
      "|    ep_rew_mean        | 124       |\n",
      "| time/                 |           |\n",
      "|    fps                | 523       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.651    |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -1.28     |\n",
      "|    value_loss         | 3.87      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 210      |\n",
      "| time/                 |          |\n",
      "|    fps                | 524      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.674   |\n",
      "|    explained_variance | -0.982   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.447    |\n",
      "|    value_loss         | 0.81     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 210      |\n",
      "| time/                 |          |\n",
      "|    fps                | 530      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.539   |\n",
      "|    explained_variance | -0.00535 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.466    |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 210      |\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.524   |\n",
      "|    explained_variance | -0.0118  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.328    |\n",
      "|    value_loss         | 1.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 210      |\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0231  |\n",
      "|    value_loss         | 2.74     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 264      |\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.393   |\n",
      "|    explained_variance | -0.365   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.396    |\n",
      "|    value_loss         | 9.86     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 264      |\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.478   |\n",
      "|    explained_variance | 4.62e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -1.37    |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 264      |\n",
      "| time/                 |          |\n",
      "|    fps                | 539      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.17     |\n",
      "|    value_loss         | 0.477    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 264      |\n",
      "| time/                 |          |\n",
      "|    fps                | 536      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.582   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 2.3      |\n",
      "|    value_loss         | 6.88     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.629   |\n",
      "|    explained_variance | 0.152    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.838   |\n",
      "|    value_loss         | 2.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0.0162   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.0964   |\n",
      "|    value_loss         | 2.95     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.395   |\n",
      "|    explained_variance | 5.96e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.406    |\n",
      "|    value_loss         | 2.51     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 353      |\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.348   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.359    |\n",
      "|    value_loss         | 9.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | -0.218   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.353   |\n",
      "|    value_loss         | 0.687    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.99e+03  |\n",
      "|    ep_rew_mean        | 417       |\n",
      "| time/                 |           |\n",
      "|    fps                | 523       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.319    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 0.157     |\n",
      "|    value_loss         | 2.33      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 523      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.466   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 1.67     |\n",
      "|    value_loss         | 9.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 522      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.043   |\n",
      "|    value_loss         | 2.36     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.99e+03  |\n",
      "|    ep_rew_mean        | 452       |\n",
      "| time/                 |           |\n",
      "|    fps                | 521       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.209    |\n",
      "|    explained_variance | -0.000416 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 0.239     |\n",
      "|    value_loss         | 9.38      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 452      |\n",
      "| time/                 |          |\n",
      "|    fps                | 521      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.339    |\n",
      "|    value_loss         | 2.3      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 452      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 2.26     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 452      |\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.224   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.0525  |\n",
      "|    value_loss         | 2.22     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 542      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.601   |\n",
      "|    explained_variance | 0.0542   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.172    |\n",
      "|    value_loss         | 0.669    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 542      |\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.245   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -1.4     |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 542      |\n",
      "| time/                 |          |\n",
      "|    fps                | 517      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0952  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.0381   |\n",
      "|    value_loss         | 2.58     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 542      |\n",
      "| time/                 |          |\n",
      "|    fps                | 515      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0649  |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 0.013    |\n",
      "|    value_loss         | 1.94     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 618      |\n",
      "| time/                 |          |\n",
      "|    fps                | 508      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.222   |\n",
      "|    explained_variance | 0.553    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 1.04     |\n",
      "|    value_loss         | 4.09     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.99e+03  |\n",
      "|    ep_rew_mean        | 618       |\n",
      "| time/                 |           |\n",
      "|    fps                | 507       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.337    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 0.583     |\n",
      "|    value_loss         | 5.71      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 618      |\n",
      "| time/                 |          |\n",
      "|    fps                | 507      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.177   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 0.0362   |\n",
      "|    value_loss         | 1.32     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 618      |\n",
      "| time/                 |          |\n",
      "|    fps                | 507      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0488  |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 2.37     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 668      |\n",
      "| time/                 |          |\n",
      "|    fps                | 506      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.346   |\n",
      "|    explained_variance | 8e-05    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.125   |\n",
      "|    value_loss         | 0.92     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 668      |\n",
      "| time/                 |          |\n",
      "|    fps                | 503      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.337   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.0867  |\n",
      "|    value_loss         | 4.45     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 668      |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0391  |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.0175   |\n",
      "|    value_loss         | 7.96     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 668      |\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.0354   |\n",
      "|    value_loss         | 2.43     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.99e+03 |\n",
      "|    ep_rew_mean        | 711      |\n",
      "| time/                 |          |\n",
      "|    fps                | 494      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | 2.26e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.0285  |\n",
      "|    value_loss         | 0.892    |\n",
      "------------------------------------\n",
      "{'tp_rate': 82.15, 'fp_rate': 9.4, 'episode': {'r': 1097.616062, 'l': 1990, 't': 40.182701}, 'TimeLimit.truncated': False, 'terminal_observation': array([-2.9579135e-02, -5.3094257e-02, -4.7320537e-02,  1.0000000e+02,\n",
      "       -4.8418212e+00,  8.5814774e+01,  3.3032751e-01, -2.0016024e+00,\n",
      "       -2.5014141e-01, -1.8992467e+00,  1.0000000e+02,  6.7782421e+00,\n",
      "        3.9088852e+01,  2.7170370e+00, -2.4204135e+00, -3.0400717e-01,\n",
      "       -2.4657650e+00,  1.0000000e+02,  1.7051378e+01,  3.6368351e+01,\n",
      "       -2.1386948e+00, -2.0897760e+00, -3.0210674e-01, -2.1208229e+00,\n",
      "        1.0000000e+02,  2.0644293e+01,  4.1278316e+01,  1.1793755e+00],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "env = FrequencyDetectionEnv(data)\n",
    "model_a2c = A2C('MlpPolicy', env, verbose=1#, learning_rate=5*1e-4,gamma=0.9,\n",
    "            #exploration_fraction= 0.1, exploration_initial_eps = 1, exploration_final_eps = 0.05\n",
    "            ,policy_kwargs=dict(activation_fn=th.nn.ReLU,net_arch=[data.shape[2]+1])\n",
    "            )\n",
    "model_a2c.learn(total_timesteps=timesteps)\n",
    "model_a2c.save('mini_a2c_snr5.zip')\n",
    "_, latest = env.reset()\n",
    "print(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL 2 (exp data, filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAL_FrequencyDetectionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Frequency Objective Response Detection that follows gymnasium interface\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(EVAL_FrequencyDetectionEnv, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.episodes =  data.shape[2]-1\n",
    "        self.frequencies = data.shape[0]-1\n",
    "        self.measures = data.shape[1]\n",
    "        self.windows = data.shape[2]-1\n",
    "        self.current_episode = 0\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "\n",
    "        # Action space: 0 (no detection) or 1 (detection)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation space: 5 measures for each of the 4 frequencies (1 focus + 3 noise)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4*self.measures,), dtype=np.float32)\n",
    "\n",
    "        # Initialize the internal state\n",
    "        self.state = None\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0   \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []  \n",
    "        self.latest_result = {'tp_rate':np.nan,'fp_rate':np.nan}   \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to an initial state and returns the initial observation.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if  self.current_episode< self.episodes:\n",
    "            self.current_episode +=1\n",
    "        else:\n",
    "            self.current_episode = np.random.randint(0, self.episodes)\n",
    "\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0        \n",
    "         \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        \n",
    "        # Initialize the state\n",
    "        self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "        return self.state, self.latest_result\n",
    "\n",
    "    def _get_state(self, frequency, window, episode= 0):\n",
    "        \"\"\"Helper function to get the state at a given frequency and window.\"\"\"\n",
    "         \n",
    "        if episode != 0:\n",
    "            self.current_episode = episode\n",
    "         \n",
    "        # print(frequency)\n",
    "        focus_measure = self.data[frequency, :, window]\n",
    "        \n",
    "        # Select 3 random noise frequencies, excluding the current focus frequency\n",
    "        noise_indices = np.setdiff1d(np.arange(9, 17), frequency - 9 if frequency >= 9 else [])\n",
    "        noise_frequencies = np.random.choice(noise_indices, 3, replace=False)\n",
    "        noise_measures = [self.data[nf, :, window] for nf in noise_frequencies]\n",
    "         \n",
    "        # measures = my_norm(np.concatenate(([focus_measure], noise_measures), axis=0).flatten())\n",
    "        measures = np.concatenate(([focus_measure], noise_measures), axis=0).flatten()\n",
    "       \n",
    "        return np.array(measures, dtype=np.float32)\n",
    "\n",
    "    def step(self, action, episode = -1):\n",
    "        \"\"\"Executes one time step within the environment.\"\"\"\n",
    "        # Returns:\n",
    "        # tuple (observation, reward, terminated, truncated, info).\n",
    "        # Return type:\n",
    "        # Tuple[Tuple | Dict[str, Any] | ndarray | int, float, bool, bool, Dict]\n",
    "\n",
    "        if episode != -1:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        reward = 0\n",
    "        fp_des = 0.05\n",
    "\n",
    "        should_detect = self.current_frequency <= 8\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                self.true_positives += 1\n",
    "            else:\n",
    "                self.false_positives += 1\n",
    "\n",
    "        self.current_frequency += 1\n",
    "        window_done = self.current_frequency >= self.frequencies\n",
    "        epsiode_terminated = self.current_window >= self.windows\n",
    "        info = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "        if window_done:\n",
    "            self.state = self._get_state(self.current_frequency-1, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.frequencies//2+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.frequencies//2+1)\n",
    "\n",
    "            self.tpr_hist.append(self.tp_rate)\n",
    "            self.fpr_hist.append(self.fp_rate)\n",
    "\n",
    "            if not(epsiode_terminated):\n",
    "                self.current_window += 1\n",
    "                self.current_frequency = 0\n",
    "                self.true_positives = 0\n",
    "                self.false_positives = 0\n",
    "                    \n",
    "        if epsiode_terminated:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.current_frequency+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.current_frequency+1)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "           \n",
    "            truncated = False\n",
    "\n",
    "            info = {'tp_rate':np.round(np.mean(self.tpr_hist),2),'fp_rate':np.round(np.mean(self.fpr_hist),2), \n",
    "                    'tp_hist:':self.tpr_hist,'fp_hist:':self.fpr_hist}\n",
    "            \n",
    "            # print(info)\n",
    "            self.latest_result = info\n",
    "\n",
    "            self.tpr_hist = []\n",
    "            self.fpr_hist = [] \n",
    "            self.current_window = 2\n",
    "            self.current_frequency = 0\n",
    "            self.true_positives = 0\n",
    "            self.false_positives = 0\n",
    "\n",
    "        else:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "            truncated = False\n",
    "\n",
    "        reward = tp-fp-fn+tn\n",
    "\n",
    "        return self.state, reward, epsiode_terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment for visualization purposes.\"\"\"\n",
    "        if mode =='human':\n",
    "            print(f'Window: {self.current_window}, Frequency: {self.current_frequency},TP: {self.tp_rate}, FP: {self.fp_rate}')\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up any resources used by the environment.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5), (10, 1), (10, 2), (10, 3), (10, 4), (10, 5), (11, 1), (11, 2), (11, 3), (11, 4), (11, 5)])\n",
      "(18, 7, 58)\n"
     ]
    }
   ],
   "source": [
    "with open('c_states_dict_exp_filt.pkl', 'rb') as f:\n",
    "    c_states_exp_filt = pickle.load(f)\n",
    "\n",
    "print(c_states_exp_filt.keys())\n",
    "print(c_states_exp_filt[1,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062\n"
     ]
    }
   ],
   "source": [
    "print((data.shape[2]+1)*(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ivol in range(1,12):\n",
    "    for iint in range(1,6):\n",
    "        c_states_exp_filt[ivol,iint] = np.nan_to_num(c_states_exp_filt[ivol,iint], nan=0, posinf=10, neginf=-10)\n",
    "\n",
    "# Validate the trained model\n",
    "intensidades = ['70','60','50','40','30']\n",
    "\n",
    "for iint in range(1,6):\n",
    "    print(f'Intensidade: {intensidades[iint-1]} dB')\n",
    "    for ivol in range(1,12):\n",
    "        \n",
    "        data = c_states_exp_filt[ivol,iint] \n",
    "        print(data.shape)\n",
    "        env = EVAL_FrequencyDetectionEnv(data)\n",
    "        obs, info = env.reset()\n",
    "        # model = PPO.load('ppo_snr0-5.zip', env=env)\n",
    "        # model = DQN.load('dqn_snr0-5.zip', env=env)\n",
    "        \n",
    "        # model = DQN.load('dqn_snr0_306000steps_308eps.zip', env=env)\n",
    "\n",
    "        model = PPO.load('mini_ppo_snr5.zip', env=env)\n",
    "\n",
    "        # model.learn(total_timesteps=(data.shape[2]-1-5)*(data.shape[0]-1))\n",
    "        # model.learn(total_timesteps=1)\n",
    "        # obs, info = env.reset()\n",
    "        # print(info)\n",
    "        \n",
    "        for test_episode in range((data.shape[2]-2)*(data.shape[0]-1)):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if terminated:\n",
    "                obs, info = env.reset()\n",
    "                print(info)\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensidade: 70 dB\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "TPR:85.56545454545453\n",
      "FPR:30.101818181818178\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ivol in range(1,12):\n",
    "    for iint in range(1,6):\n",
    "        c_states_exp_filt[ivol,iint] = np.nan_to_num(c_states_exp_filt[ivol,iint], nan=0, posinf=10, neginf=-10)\n",
    "\n",
    "# Validate the trained model\n",
    "intensidades = ['70','60','50','40','30']\n",
    "hist_tp = []\n",
    "hist_fp = [] \n",
    "\n",
    "for iint in range(1,2):\n",
    "    print(f'Intensidade: {intensidades[iint-1]} dB')\n",
    "    for ivol in range(1,12):\n",
    "        \n",
    "        data = c_states_exp_filt[ivol,iint] \n",
    "        env = EVAL_FrequencyDetectionEnv(data)\n",
    "        obs, info = env.reset()\n",
    "        model = DQN.load('mini_dqn_snr5.zip', env=env)\n",
    "\n",
    "        for test_episode in range((data.shape[2]-1)*(data.shape[0]-1)):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if terminated:\n",
    "                obs, info = env.reset()\n",
    "                hist_tp.append(info['tp_rate'])\n",
    "                hist_fp.append(info['fp_rate'])\n",
    "                # print(info)\n",
    "            env.close()\n",
    "\n",
    "print(f'TPR:{np.mean(hist_tp)}')\n",
    "print(f'FPR:{np.mean(hist_fp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensidade: 70 dB\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.79, 'fp_rate': 23.03}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.19, 'fp_rate': 36.36}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.99, 'fp_rate': 43.03}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.79, 'fp_rate': 25.05}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.79, 'fp_rate': 23.43}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.18, 'fp_rate': 46.26}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 97.58, 'fp_rate': 48.08}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.39, 'fp_rate': 23.03}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.59, 'fp_rate': 37.37}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.79, 'fp_rate': 22.22}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.39, 'fp_rate': 25.86}\n",
      "Intensidade: 60 dB\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.86, 'fp_rate': 24.06}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.86, 'fp_rate': 25.9}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.17, 'fp_rate': 45.59}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.86, 'fp_rate': 24.63}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 97.84, 'fp_rate': 28.57}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.11, 'fp_rate': 38.41}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.86, 'fp_rate': 40.76}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.11, 'fp_rate': 23.94}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.73, 'fp_rate': 42.22}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.03, 'fp_rate': 25.12}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.65, 'fp_rate': 26.86}\n",
      "Intensidade: 50 dB\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.69, 'fp_rate': 25.64}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.64, 'fp_rate': 28.47}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.32, 'fp_rate': 44.29}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.79, 'fp_rate': 25.99}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 98.61, 'fp_rate': 25.46}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "{'tp_rate': 99.44, 'fp_rate': 42.6}\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m A2C\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_a2c_snr5.zip\u001b[39m\u001b[38;5;124m'\u001b[39m, env\u001b[38;5;241m=\u001b[39menv)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m((data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 18\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:751\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03mGet the current policy distribution given the observations.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m:param obs:\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m:return: the action distribution.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    750\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n\u001b[1;32m--> 751\u001b[0m latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\epd_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for ivol in range(1,12):\n",
    "    for iint in range(1,6):\n",
    "        c_states_exp_filt[ivol,iint] = np.nan_to_num(c_states_exp_filt[ivol,iint], nan=0, posinf=10, neginf=-10)\n",
    "\n",
    "# Validate the trained model\n",
    "intensidades = ['70','60','50','40','30']\n",
    "\n",
    "for iint in range(1,6):\n",
    "    print(f'Intensidade: {intensidades[iint-1]} dB')\n",
    "    for ivol in range(1,12):\n",
    "        \n",
    "        data = c_states_exp_filt[ivol,iint] \n",
    "        env = EVAL_FrequencyDetectionEnv(data)\n",
    "        obs, info = env.reset()\n",
    "        model = A2C.load('mini_a2c_snr5.zip', env=env)\n",
    "\n",
    "        for test_episode in range((data.shape[2]-1)*(data.shape[0]-1)):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if terminated:\n",
    "                obs, info = env.reset()\n",
    "                print(info)\n",
    "            env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
