{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado por Reforço para Detecção de Respostas Cerebrais a Estímulo Auditivo\n",
    "*Aluno: [Alexandre Gomes Caldeira](https://orcid.org/0000-0002-4851-3417); Orientador: [Leonardo Bonato Felix](https://orcid.org/0000-0002-6184-2354)*.\n",
    "\n",
    "**Disponível em [github.com/Alexandre-Caldeira/drl-ord/blob/main/DRL_ORD.ipynb](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/DRL_ORD.ipynb), e [pitch no Google Drive](link) <sup>1,2</sup>.**\n",
    "\n",
    "Artigos produzidos com os resultados discutidos aqui:\n",
    "1. [Patient-adaptive Objective Response Detection using Reinforcement Learning <br> (Aceito para publicação, CBEB 2024)](https://drive.google.com/file/d/191mQeyK0PxDVYrpS6Daull_epmzeavzT/view?usp=sharing)\n",
    "\n",
    "2. [Evaluating Deep Reinforcement Learning on Auditory Steady State Response Detection <br> (Em submissão para publicação)](https://drive.google.com/file/d/1ykkx8_rxxhN6wHBJBFsyJ2YF8ANYEkGY/view?usp=sharing)\n",
    "\n",
    "Respositórios com código:\n",
    "\n",
    "**Artigo 1: [Alexandre-Caldeira/CBEB24-RL-ORD](https://github.com/Alexandre-Caldeira/CBEB24-RL-ORD)**\n",
    "\n",
    "**Artigo 2: [Alexandre-Caldeira/drl-ord](https://github.com/Alexandre-Caldeira/drl-ord)**\n",
    "\n",
    "\n",
    "**<sup>1</sup>** *Aqui é apresentado um overview resumido do problema e dados, da arquitetura, treino e validação dos modelos, mas o notebook com treino completo e resultados extendidos pode ser acessado em [comparing_3_models.ipynb](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/epd-kit/draft_notebooks/comparing_3_models.ipynb) (tempo de execução de aprox. 48hrs).*\n",
    "\n",
    "**<sup>2</sup>** *Para replicar o ambiente exato de execução desse notebook, siga os passos descritos no tutorial em [arch.md](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/arch.md).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "\n",
    "## Manipulação de dados\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Aprendizado de máquina\n",
    "import torch as th\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, DQN, A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O problema e os dados\n",
    "\n",
    "Se oferecido estímulo sonoro a seres humanos, é possível detectar respostas elétricas medidas no escalpo quando não há perda auditiva. Porém, eletroencefalograma não-invasivo é extremamente ruidoso. \n",
    "\n",
    "![](https://github.com/Alexandre-Caldeira/CBEB24-RL-ORD/blob/main/paper_figures/eeg_simulation_frequency.png?raw=true)\n",
    "\n",
    "Nesse trabalho, simulamos respostas auditivas em estado permanente (ASSRs) a diferentes níveis de relação sinal-ruído (SNR), para ajustarmos detectores e aplicarmos em dados experimentais com voluntários de audição normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega e apresenta exemplo de dados sintéticos em diferentes SNR\n",
    "## Método de simulação disponível em ./matlab_sim_sourcecode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A arquitetura e método de solução\n",
    "\n",
    "Seguindo resultados a nível do estado da arte utilizando [Q-Learning tabular](https://github.com/Alexandre-Caldeira/CBEB24-RL-ORD), propomos aqui comparar detectores clássicos com métodos de Aprendizado Profundo por Reforço na tarefa de detecção de resposta objetiva a estímulo auditivo em estado permanente.\n",
    "\n",
    "Selecionamos três abordagens: Deep Q-Networks (DQN), Advantage Actor Critic (A2C) e Proximal Policy Optimization (PPO) a partir de sua arquitetura e implementação conforme publicada, disponível na biblioteca [stable-baselines3](https://github.com/DLR-RM/stable-baselines3/). \n",
    "\n",
    "Os agentes tem duas ações: detectar (1) ou não (0) uma resposta, dado um estado composto por medidas de resposta objetiva no domínio da frequência, utilizados por detectores clássicos da literatura. Suas recompensas devem ser baseadas na adequação da sua decisão ao estado atual, nominalmente se sua detecção representa um Verdadeiro Positivo (TP), Verdadeiro Negativo (TN), Falso Positivo (FP) ou Falso Negativo (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados e próximos passos\n",
    "\n",
    "Etapas:\n",
    "1. **Pré-treino** com dados sintéticos para aprendizado de representação profunda a diferentes níveis de ruído.\n",
    "\n",
    "2. **Teste** em diversos níveis de ruído (incluindo ainda não vistos), comparando com detectores clássicos.\n",
    "\n",
    "3. **Validação cruzada** com dados experimentais utilizando 1 exemplo de exame como treino e 10 exames como teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
