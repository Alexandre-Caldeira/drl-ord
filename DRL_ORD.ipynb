{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado Profundo por Reforço para Detecção de Respostas Cerebrais a Estímulo Auditivo\n",
    "*Aluno: [Alexandre Gomes Caldeira](https://orcid.org/0000-0002-4851-3417); Orientador: [Leonardo Bonato Felix](https://orcid.org/0000-0002-6184-2354)*.\n",
    "\n",
    "**Disponível em [github.com/Alexandre-Caldeira/drl-ord/blob/main/DRL_ORD.ipynb](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/DRL_ORD.ipynb), e [pitch no Google Drive](link) <sup>1,2</sup>.**\n",
    "\n",
    "Artigos produzidos com os resultados discutidos aqui:\n",
    "1. [Patient-adaptive Objective Response Detection using Reinforcement Learning <br> (Aceito para publicação, CBEB 2024)](https://drive.google.com/file/d/191mQeyK0PxDVYrpS6Daull_epmzeavzT/view?usp=sharing)\n",
    "\n",
    "2. [Evaluating Deep Reinforcement Learning on Auditory Steady State Response Detection <br> (Em submissão para publicação)](https://drive.google.com/file/d/1ykkx8_rxxhN6wHBJBFsyJ2YF8ANYEkGY/view?usp=sharing)\n",
    "\n",
    "Respositórios com código:\n",
    "\n",
    "**Artigo 1: [Alexandre-Caldeira/CBEB24-RL-ORD](https://github.com/Alexandre-Caldeira/CBEB24-RL-ORD)**\n",
    "\n",
    "**Artigo 2: [Alexandre-Caldeira/drl-ord](https://github.com/Alexandre-Caldeira/drl-ord)**\n",
    "\n",
    "\n",
    "**<sup>1</sup>** *Aqui é apresentado um overview resumido do problema e dados, da arquitetura, treino e validação dos modelos, mas o notebook com treino completo e resultados extendidos pode ser acessado em [comparing_3_models.ipynb](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/epd-kit/draft_notebooks/comparing_3_models.ipynb) (tempo de execução de aprox. 48hrs).*\n",
    "\n",
    "**<sup>2</sup>** *Para replicar o ambiente exato de execução desse notebook, siga os passos descritos no tutorial em [arch.md](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/arch.md).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega bibliotecas necessárias\n",
    "\n",
    "## Manipulação de dados\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Aprendizado de máquina\n",
    "import torch as th\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Carrega dados\n",
    "with open('./epd-kit/database/ex_signals_15.pkl', 'rb') as f:\n",
    "    signals_15 = pickle.load(f)\n",
    "\n",
    "with open('./epd-kit/database/ex_signals_10.pkl', 'rb') as f:\n",
    "    signals_10 = pickle.load(f)\n",
    "\n",
    "with open('./epd-kit/database/c_states_dict_acer_v3.pkl', 'rb') as f:\n",
    "    c_states_big = pickle.load(f)\n",
    "\n",
    "with open('./epd-kit/database/c_states_dict_exp_filt.pkl', 'rb') as f:\n",
    "    c_states_exp_filt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O problema e os dados\n",
    "\n",
    "Se oferecido estímulo sonoro a seres humanos, é possível detectar respostas elétricas medidas no escalpo quando não há perda auditiva. Porém, eletroencefalograma não-invasivo é extremamente ruidoso. \n",
    "\n",
    "![](https://github.com/Alexandre-Caldeira/CBEB24-RL-ORD/blob/main/paper_figures/eeg_simulation_frequency.png?raw=true)\n",
    "\n",
    "Nesse trabalho, simulamos respostas auditivas em estado permanente (ASSRs) a diferentes níveis de relação sinal-ruído (SNR), para ajustarmos detectores e aplicarmos em dados experimentais com voluntários de audição normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos de dados simulados e métricas    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(211)\n",
    "plt.title('SNR = 15 dB')\n",
    "plt.stem(np.abs(signals_15))\n",
    "plt.ylabel('Potência')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title('SNR = 10 dB')\n",
    "plt.stem(np.abs(signals_10))\n",
    "plt.grid(True)\n",
    "plt.ylabel('Potência')\n",
    "plt.xlabel('Frequência [Hz]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega e apresenta exemplo de dados sintéticos em diferentes SNR\n",
    "## Método de simulação disponível em ./matlab_sim_sourcecode\n",
    "ex = c_states_big[10]\n",
    "print(ex.shape)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.stem(10**ex[1,:,1,1])\n",
    "plt.xticks(np.arange(0,18))\n",
    "plt.title('Métricas diferentes auxiliam na detecção')\n",
    "plt.ylabel('Magnitude quadrática da coerência')\n",
    "plt.xlabel('Índice da frequência simulada')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A arquitetura e método de solução\n",
    "\n",
    "Seguindo resultados a nível do estado da arte utilizando [Q-Learning tabular](https://github.com/Alexandre-Caldeira/CBEB24-RL-ORD), propomos comparar detectores clássicos com métodos de Aprendizado Profundo por Reforço.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Agente:** deve comparar medidas na frequência observada com referências, e decidir há resposta;\n",
    "\n",
    "**Abordagens:** Deep Q-Networks **(DQN)**, Advantage Actor Critic **(A2C)** e Proximal Policy Optimization **(PPO)**;\n",
    " - *Arquitetura e implementação conforme publicada, disponível na biblioteca [stable-baselines3](https://github.com/DLR-RM/stable-baselines3/)*.\n",
    "\n",
    "**Ações:** detectar (1) ou não (0) uma resposta;\n",
    "\n",
    "**Estado:** medidas de resposta objetiva para 1 frequência de decisão e 3 frequências de referência;\n",
    "\n",
    "**Recompensas:** uma imediata **(Ra)**, e outra ao fim da janela de decisão **(Rb)**;\n",
    "- **(Ra)** Verdadeiro Positivo (TP), Verdadeiro Negativo (TN), Falso Positivo (FP) ou Falso Negativo (FN);\n",
    "- **(Rb)** $\\Big[\\frac{(1-\\alpha)^2}{100} - \\frac{\\alpha^2}{100} \\Big] + \\Big[\\frac{TP^2}{100} -\\frac{FP^2}{100} \\Big]$; $\\alpha$ sendo FP desejado para o exame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino e Teste\n",
    "\n",
    "Em teste, não há acesso ao Verdadeiro Positivo na recompensa imediata **(Ra)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyDetectionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Frequency Objective Response Detection that follows gymnasium interface\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(FrequencyDetectionEnv, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.episodes = data.shape[0]-1\n",
    "        self.frequencies = data.shape[1]-1\n",
    "        self.measures = data.shape[2]\n",
    "        self.windows = data.shape[3]-1\n",
    "        self.current_episode = 0\n",
    "        self.current_window = 2 # Discards first 2 seconds of experiments\n",
    "        self.current_frequency = 0\n",
    "\n",
    "        # Action space: 0 (no detection) or 1 (detection)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation space: measures for each of the 4 frequencies (1 focus + 3 noise)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4*self.measures,), dtype=np.float32)\n",
    "\n",
    "        # Initialize the internal state\n",
    "        self.state = None\n",
    "        self.fp_des = 0.05\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0   \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        self.latest_result = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to an initial state and returns the initial observation.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if  self.current_episode< self.episodes:\n",
    "            self.current_episode +=1\n",
    "        else:\n",
    "            self.current_episode = np.random.randint(0, self.episodes)\n",
    "\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0        \n",
    "         \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        \n",
    "        # Initialize the state\n",
    "        self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "        return self.state, self.latest_result\n",
    "\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment for visualization purposes.\"\"\"\n",
    "        if mode=='human':\n",
    "            print(f'Window: {self.current_window}, Frequency: {self.current_frequency},TP: {self.tp_rate}, FP: {self.fp_rate}')\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up any resources used by the environment.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_state(self, frequency, window, episode= 0):\n",
    "        \"\"\"Helper function to get the state at a given frequency and window.\"\"\"\n",
    "         \n",
    "        if episode != 0:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        focus_measure = self.data[self.current_episode, frequency, :, window]\n",
    "        \n",
    "        # Select 3 random noise frequencies, excluding the current focus frequency\n",
    "        noise_indices = np.arange(9, 17)\n",
    "        noise_frequencies = np.random.choice(noise_indices, 3, replace=False)\n",
    "        noise_measures = [self.data[self.current_episode, nf, :, window] for nf in noise_frequencies]\n",
    "         \n",
    "        measures = np.concatenate(([focus_measure], noise_measures), axis=0).flatten()\n",
    "\n",
    "        return np.array(measures, dtype=np.float32)\n",
    "\n",
    "    def step(self, action, episode = -1):\n",
    "        \"\"\"Executes one time step within the environment.\"\"\"\n",
    "        # Returns:\n",
    "        # tuple (observation, reward, terminated, truncated, info).\n",
    "        # Return type:\n",
    "        # Tuple[Tuple | Dict[str, Any] | ndarray | int, float, bool, bool, Dict]\n",
    "        \n",
    "        if episode != -1:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # Frequencies above index 8 are noise only (but agent doesn't know)\n",
    "        should_detect = self.current_frequency <= 8\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                self.true_positives += 1\n",
    "            else:\n",
    "                self.false_positives += 1\n",
    "\n",
    "        ### Instant reward (Ra)\n",
    "        tp = 0; fp = 0; fn =0; tn = 0\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                tp = 0\n",
    "            else:\n",
    "                fp = 1\n",
    "        else:\n",
    "            if should_detect:\n",
    "                fn = 0\n",
    "            else:\n",
    "                tn = 1\n",
    "\n",
    "        reward = tp-fp-fn+tn\n",
    "\n",
    "        \n",
    "        window_done = self.current_frequency >= self.frequencies\n",
    "        episode_terminated = self.current_window >= self.windows\n",
    "        self.current_frequency += 1\n",
    "        info = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "\n",
    "        if window_done:\n",
    "            self.state = self._get_state(self.current_frequency-1, self.current_window)\n",
    "            \n",
    "            self.tp_rate = 100*self.true_positives / (self.frequencies//2+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.frequencies//2+1)\n",
    "                \n",
    "            reward = +( -(((100-self.fp_des)**2)/100 +(self.fp_des**2)/-100) +((self.fp_rate)**2)/(-100)+ ((self.tp_rate)**2)/(100) )/100\n",
    "            \n",
    "            self.tpr_hist.append(self.tp_rate)\n",
    "            self.fpr_hist.append(self.fp_rate)\n",
    "\n",
    "            if not(episode_terminated):\n",
    "                self.current_window += 1\n",
    "                self.current_frequency = 0\n",
    "                self.true_positives = 0\n",
    "                self.false_positives = 0\n",
    "                    \n",
    "        if episode_terminated:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.current_frequency+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.current_frequency+1)\n",
    "\n",
    "            ### Long term reward (Rb)\n",
    "            reward = +( -(((100-self.fp_des)**2)/100+(self.fp_des**2)/-100)\n",
    "                    +((np.round(np.mean(self.fpr_hist),4))**2)/(-100)\n",
    "                    +((np.round(np.mean(self.tpr_hist),4))**2)/(100))/100\n",
    "           \n",
    "            truncated = False\n",
    "\n",
    "            info = {'tp_rate':np.round(np.mean(self.tpr_hist),2),'fp_rate':np.round(np.mean(self.fpr_hist),2)}\n",
    "            self.latest_result = info\n",
    "\n",
    "            self.tpr_hist = []\n",
    "            self.fpr_hist = [] \n",
    "            self.current_window = 2\n",
    "            self.current_frequency = 0\n",
    "            self.true_positives = 0\n",
    "            self.false_positives = 0\n",
    "\n",
    "        else:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 1\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 1\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "            truncated = False\n",
    "\n",
    "        return self.state, reward, episode_terminated, truncated, info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST_FrequencyDetectionEnv(FrequencyDetectionEnv):\n",
    "    def __init__(self, data):\n",
    "        super(TEST_FrequencyDetectionEnv, self).__init__(data)\n",
    "\n",
    "    def step(self, action, episode = -1):\n",
    "\n",
    "        if episode != -1:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        should_detect = self.current_frequency <= 8\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                self.true_positives += 1\n",
    "            else:\n",
    "                self.false_positives += 1\n",
    "\n",
    "        self.current_frequency += 1\n",
    "        window_done = self.current_frequency >= self.frequencies\n",
    "        episode_terminated = self.current_window >= self.windows\n",
    "        info = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "\n",
    "        if window_done:\n",
    "            self.state = self._get_state(self.current_frequency-1, self.current_window)\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.frequencies//2+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.frequencies//2+1)\n",
    "\n",
    "            self.tpr_hist.append(self.tp_rate)\n",
    "            self.fpr_hist.append(self.fp_rate)\n",
    "\n",
    "            if not(episode_terminated):\n",
    "                self.current_window += 1\n",
    "                self.current_frequency = 0\n",
    "                self.true_positives = 0\n",
    "                self.false_positives = 0\n",
    "                    \n",
    "        if episode_terminated:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.current_frequency+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.current_frequency+1)\n",
    "\n",
    "            info = {'tp_rate':np.round(np.mean(self.tpr_hist),2),'fp_rate':np.round(np.mean(self.fpr_hist),2)}\n",
    "            self.latest_result = info\n",
    "\n",
    "            self.tpr_hist = []\n",
    "            self.fpr_hist = [] \n",
    "            self.current_window = 2\n",
    "            self.current_frequency = 0\n",
    "            self.true_positives = 0\n",
    "            self.false_positives = 0\n",
    "\n",
    "        else:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "        tp = 0; fp = 0; fn =0; tn = 0\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                tp = 0\n",
    "            else:\n",
    "                fp = 1\n",
    "        else:\n",
    "            if should_detect:\n",
    "                fn = 0\n",
    "            else:\n",
    "                tn = 1\n",
    "\n",
    "        reward = tp-fp-fn+tn\n",
    "\n",
    "        return self.state, reward, episode_terminated, False, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning e validação\n",
    "Em validação, há somente recompensa imediata **(Ra)** e também não há acesso ao Verdadeiro Positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAL_FINETUNE_FrequencyDetectionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Frequency Objective Response Detection that follows gymnasium interface\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(EVAL_FINETUNE_FrequencyDetectionEnv, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.episodes =  data.shape[2]-1\n",
    "        self.frequencies = data.shape[0]-1\n",
    "        self.measures = data.shape[1]\n",
    "        self.windows = data.shape[2]-1\n",
    "        self.current_episode = 0\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "\n",
    "        # Action space: 0 (no detection) or 1 (detection)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation space: 5 measures for each of the 4 frequencies (1 focus + 3 noise)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4*self.measures,), dtype=np.float32)\n",
    "\n",
    "        # Initialize the internal state\n",
    "        self.state = None\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0   \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []  \n",
    "        self.latest_result = {'tp_rate':np.nan,'fp_rate':np.nan}   \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to an initial state and returns the initial observation.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if  self.current_episode< self.episodes:\n",
    "            self.current_episode +=1\n",
    "        else:\n",
    "            self.current_episode = np.random.randint(0, self.episodes)\n",
    "\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0        \n",
    "         \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        \n",
    "        # Initialize the state\n",
    "        self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "        return self.state, self.latest_result\n",
    "\n",
    "    def _get_state(self, frequency, window, episode= 0):\n",
    "        \"\"\"Helper function to get the state at a given frequency and window.\"\"\"\n",
    "         \n",
    "        if episode != 0:\n",
    "            self.current_episode = episode\n",
    "         \n",
    "        # print(frequency)\n",
    "        focus_measure = self.data[frequency, :, window]\n",
    "        \n",
    "        # Select 3 random noise frequencies, excluding the current focus frequency\n",
    "        noise_indices = np.setdiff1d(np.arange(9, 17), frequency - 9 if frequency >= 9 else [])\n",
    "        noise_frequencies = np.random.choice(noise_indices, 3, replace=False)\n",
    "        noise_measures = [self.data[nf, :, window] for nf in noise_frequencies]\n",
    "         \n",
    "        # measures = my_norm(np.concatenate(([focus_measure], noise_measures), axis=0).flatten())\n",
    "        measures = np.concatenate(([focus_measure], noise_measures), axis=0).flatten()\n",
    "       \n",
    "        return np.array(measures, dtype=np.float32)\n",
    "\n",
    "    def step(self, action, episode = -1):\n",
    "        \"\"\"Executes one time step within the environment.\"\"\"\n",
    "        # Returns:\n",
    "        # tuple (observation, reward, terminated, truncated, info).\n",
    "        # Return type:\n",
    "        # Tuple[Tuple | Dict[str, Any] | ndarray | int, float, bool, bool, Dict]\n",
    "\n",
    "        if episode != -1:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        reward = 0\n",
    "        fp_des = 0.05\n",
    "\n",
    "        should_detect = self.current_frequency <= 8\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                self.true_positives += 1\n",
    "            else:\n",
    "                self.false_positives += 1\n",
    "\n",
    "        self.current_frequency += 1\n",
    "        window_done = self.current_frequency >= self.frequencies\n",
    "        epsiode_terminated = self.current_window >= self.windows\n",
    "        info = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "        if window_done:\n",
    "            self.state = self._get_state(self.current_frequency-1, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.frequencies//2+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.frequencies//2+1)\n",
    "\n",
    "            reward = +( -(((100-fp_des)**2)/100 +(fp_des**2)/-100) +((self.fp_rate)**2)/(-100)+ ((self.tp_rate)**2)/(100) )\n",
    "            \n",
    "            self.tpr_hist.append(self.tp_rate)\n",
    "            self.fpr_hist.append(self.fp_rate)\n",
    "\n",
    "            if not(epsiode_terminated):\n",
    "                self.current_window += 1\n",
    "                self.current_frequency = 0\n",
    "                self.true_positives = 0\n",
    "                self.false_positives = 0\n",
    "                    \n",
    "        if epsiode_terminated:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.current_frequency+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.current_frequency+1)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "            \n",
    "            reward = +( -(((100-fp_des)**2)/100+(fp_des**2)/-100)\n",
    "                    +((np.round(np.mean(self.fpr_hist),4))**2)/(-100)\n",
    "                    +((np.round(np.mean(self.tpr_hist),4))**2)/(100))\n",
    "           \n",
    "            truncated = False\n",
    "\n",
    "            info = {'tp_rate':np.round(np.mean(self.tpr_hist),2),'fp_rate':np.round(np.mean(self.fpr_hist),2), \n",
    "                    'tp_hist:':self.tpr_hist,'fp_hist:':self.fpr_hist}\n",
    "            \n",
    "            # print(info)\n",
    "            self.latest_result = info\n",
    "\n",
    "            self.tpr_hist = []\n",
    "            self.fpr_hist = [] \n",
    "            self.current_window = 2\n",
    "            self.current_frequency = 0\n",
    "            self.true_positives = 0\n",
    "            self.false_positives = 0\n",
    "\n",
    "        else:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 1\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 1\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "            truncated = False\n",
    "\n",
    "        # reward = tp-fp-fn+tn\n",
    "\n",
    "        return self.state, reward, epsiode_terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment for visualization purposes.\"\"\"\n",
    "        if mode =='human':\n",
    "            print(f'Window: {self.current_window}, Frequency: {self.current_frequency},TP: {self.tp_rate}, FP: {self.fp_rate}')\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up any resources used by the environment.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAL_EXP_FrequencyDetectionEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Frequency Objective Response Detection that follows gymnasium interface\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super(EVAL_EXP_FrequencyDetectionEnv, self).__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.episodes =  data.shape[2]-1\n",
    "        self.frequencies = data.shape[0]-1\n",
    "        self.measures = data.shape[1]\n",
    "        self.windows = data.shape[2]-1\n",
    "        self.current_episode = 0\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "\n",
    "        # Action space: 0 (no detection) or 1 (detection)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation space: 5 measures for each of the 4 frequencies (1 focus + 3 noise)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4*self.measures,), dtype=np.float32)\n",
    "\n",
    "        # Initialize the internal state\n",
    "        self.state = None\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0   \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []  \n",
    "        self.latest_result = {'tp_rate':np.nan,'fp_rate':np.nan}   \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Resets the environment to an initial state and returns the initial observation.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        if  self.current_episode< self.episodes:\n",
    "            self.current_episode +=1\n",
    "        else:\n",
    "            self.current_episode = np.random.randint(0, self.episodes)\n",
    "\n",
    "        self.current_window = 2\n",
    "        self.current_frequency = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_positives = 0\n",
    "        self.fp_rate = 0\n",
    "        self.tp_rate = 0        \n",
    "         \n",
    "        self.tpr_hist = []\n",
    "        self.fpr_hist = []\n",
    "        \n",
    "        # Initialize the state\n",
    "        self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "        return self.state, self.latest_result\n",
    "\n",
    "    def _get_state(self, frequency, window, episode= 0):\n",
    "        \"\"\"Helper function to get the state at a given frequency and window.\"\"\"\n",
    "         \n",
    "        if episode != 0:\n",
    "            self.current_episode = episode\n",
    "         \n",
    "        # print(frequency)\n",
    "        focus_measure = self.data[frequency, :, window]\n",
    "        \n",
    "        # Select 3 random noise frequencies, excluding the current focus frequency\n",
    "        noise_indices = np.setdiff1d(np.arange(9, 17), frequency - 9 if frequency >= 9 else [])\n",
    "        noise_frequencies = np.random.choice(noise_indices, 3, replace=False)\n",
    "        noise_measures = [self.data[nf, :, window] for nf in noise_frequencies]\n",
    "         \n",
    "        # measures = my_norm(np.concatenate(([focus_measure], noise_measures), axis=0).flatten())\n",
    "        measures = np.concatenate(([focus_measure], noise_measures), axis=0).flatten()\n",
    "       \n",
    "        return np.array(measures, dtype=np.float32)\n",
    "\n",
    "    def step(self, action, episode = -1):\n",
    "        \"\"\"Executes one time step within the environment.\"\"\"\n",
    "        # Returns:\n",
    "        # tuple (observation, reward, terminated, truncated, info).\n",
    "        # Return type:\n",
    "        # Tuple[Tuple | Dict[str, Any] | ndarray | int, float, bool, bool, Dict]\n",
    "\n",
    "        if episode != -1:\n",
    "            self.current_episode = episode\n",
    "\n",
    "        reward = 0\n",
    "        fp_des = 0.05\n",
    "\n",
    "        should_detect = self.current_frequency <= 8\n",
    "        if action == 1:\n",
    "            if should_detect:\n",
    "                self.true_positives += 1\n",
    "            else:\n",
    "                self.false_positives += 1\n",
    "\n",
    "        self.current_frequency += 1\n",
    "        window_done = self.current_frequency >= self.frequencies\n",
    "        epsiode_terminated = self.current_window >= self.windows\n",
    "        info = {'tp_rate':np.nan,'fp_rate':np.nan}\n",
    "        if window_done:\n",
    "            self.state = self._get_state(self.current_frequency-1, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.frequencies//2+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.frequencies//2+1)\n",
    "\n",
    "            reward = +( -(((100-fp_des)**2)/100 +(fp_des**2)/-100) +((self.fp_rate)**2)/(-100)+ ((self.tp_rate)**2)/(100) )\n",
    "            \n",
    "            self.tpr_hist.append(self.tp_rate)\n",
    "            self.fpr_hist.append(self.fp_rate)\n",
    "\n",
    "            if not(epsiode_terminated):\n",
    "                self.current_window += 1\n",
    "                self.current_frequency = 0\n",
    "                self.true_positives = 0\n",
    "                self.false_positives = 0\n",
    "                    \n",
    "        if epsiode_terminated:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            self.tp_rate = 100*self.true_positives / (self.current_frequency+1)\n",
    "            self.fp_rate = 100*self.false_positives / (self.current_frequency+1)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "            \n",
    "            reward = +( -(((100-fp_des)**2)/100+(fp_des**2)/-100)\n",
    "                    +((np.round(np.mean(self.fpr_hist),4))**2)/(-100)\n",
    "                    +((np.round(np.mean(self.tpr_hist),4))**2)/(100))\n",
    "           \n",
    "            truncated = False\n",
    "\n",
    "            info = {'tp_rate':np.round(np.mean(self.tpr_hist),2),'fp_rate':np.round(np.mean(self.fpr_hist),2), \n",
    "                    'tp_hist:':self.tpr_hist,'fp_hist:':self.fpr_hist}\n",
    "            \n",
    "            # print(info)\n",
    "            self.latest_result = info\n",
    "\n",
    "            self.tpr_hist = []\n",
    "            self.fpr_hist = [] \n",
    "            self.current_window = 2\n",
    "            self.current_frequency = 0\n",
    "            self.true_positives = 0\n",
    "            self.false_positives = 0\n",
    "\n",
    "        else:\n",
    "            self.state = self._get_state(self.current_frequency, self.current_window)\n",
    "\n",
    "            should_detect = self.current_frequency <= 8\n",
    "            tp = 0; fp = 0; fn =0; tn = 0\n",
    "            if action == 1:\n",
    "                if should_detect:\n",
    "                    tp = 0\n",
    "                else:\n",
    "                    fp = 1\n",
    "            else:\n",
    "                if should_detect:\n",
    "                    fn = 0\n",
    "                else:\n",
    "                    tn = 1\n",
    "\n",
    "            reward = tp-fp-fn+tn\n",
    "\n",
    "            truncated = False\n",
    "\n",
    "        # reward = tp-fp-fn+tn\n",
    "\n",
    "        return self.state, reward, epsiode_terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment for visualization purposes.\"\"\"\n",
    "        if mode =='human':\n",
    "            print(f'Window: {self.current_window}, Frequency: {self.current_frequency},TP: {self.tp_rate}, FP: {self.fp_rate}')\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up any resources used by the environment.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados e próximos passos\n",
    "\n",
    "Etapas:\n",
    "1. **Pré-treino** com dados sintéticos para aprendizado de representação profunda a diferentes níveis de ruído.\n",
    "\n",
    "2. **Teste** em diversos níveis de ruído (incluindo ainda não vistos), comparando com detectores clássicos.\n",
    "\n",
    "3. **Validação cruzada** com dados experimentais utilizando 1 exemplo de exame como treino e 10 exames como teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo demonstrativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DQN model\n",
    "data = c_states_big[10]\n",
    "timesteps = (118)*(16)*10\n",
    "\n",
    "env = FrequencyDetectionEnv(data)\n",
    "model_dqn = DQN('MlpPolicy', env, verbose=1#, learning_rate=5*1e-4,gamma=0.9,\n",
    "            #exploration_fraction= 0.1, exploration_initial_eps = 1, exploration_final_eps = 0.05\n",
    "            ,policy_kwargs=dict(activation_fn=th.nn.ReLU,net_arch=[data.shape[2]+1])\n",
    "                )\n",
    "model_dqn.learn(total_timesteps=timesteps)\n",
    "model_dqn.save('./epd-kit/models/minimal_dqn_snr10.zip')\n",
    "_, last_results = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_evaluate_model(model, env, num_episodes=50):\n",
    "    tpr_hist = []\n",
    "    fpr_hist = []\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        obs = obs[0]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        tpr_episode = []\n",
    "        fpr_episode = []\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if not np.isnan(info['tp_rate']):\n",
    "                tpr_episode.append(info['tp_rate'])\n",
    "                fpr_episode.append(info['fp_rate'])\n",
    "\n",
    "                done = True\n",
    "\n",
    "        tpr_hist.append(np.mean(tpr_episode))\n",
    "        fpr_hist.append(np.mean(fpr_episode))\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return tpr_hist, fpr_hist, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = 5\n",
    "data = c_states_big[snr]\n",
    "\n",
    "test_env = TEST_FrequencyDetectionEnv(data)\n",
    "model_dqn = DQN.load('./epd-kit/models/minimal_dqn_snr10.zip', env = test_env)\n",
    "\n",
    "# Avalia performance atual do modelo\n",
    "tpr_dqn, fpr_dqn, rewards_dqn = my_evaluate_model(model_dqn, test_env,num_episodes=10)\n",
    "pd.DataFrame({'TPR':tpr_dqn, 'FPR':fpr_dqn,'REW':rewards_dqn}).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation a 30 dB (3 mins)\n",
    "# Function to fine-tune the model\n",
    "def fine_tune_model(model_class, pretrained_model_path, save_model_path, data, reset_model):\n",
    "    env = EVAL_FINETUNE_FrequencyDetectionEnv(data)\n",
    "    \n",
    "    if reset_model == True:  # On first run, initialize model as copy of pretrained\n",
    "        model = model_class.load(pretrained_model_path, env=env)\n",
    "    else:\n",
    "        model = model_class.load(save_model_path, env=env)\n",
    "\n",
    "    model.learn((data.shape[2] - 1) * (data.shape[0] - 1))\n",
    "    model.save(save_model_path)\n",
    "    return model\n",
    "\n",
    "# Function to validate the model\n",
    "def validate_model(model_class, save_model_path, data, hist_tpr, hist_fpr):\n",
    "    env = EVAL_EXP_FrequencyDetectionEnv(data)\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    model = model_class.load(save_model_path, env=env)\n",
    "\n",
    "    for test_episode in range((data.shape[2] - 1) * (data.shape[0] - 1)):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            obs, info = env.reset()\n",
    "            hist_tpr.append(info['tp_rate'])\n",
    "            hist_fpr.append(info['fp_rate'])\n",
    "            env.close()\n",
    "\n",
    "    return hist_tpr,hist_fpr\n",
    "\n",
    "# Function to run cross-validation\n",
    "def cross_validate_model(model_class, pretrained_model_path, save_model_path, k_folds=5):\n",
    "\n",
    "    # cv_results = {fold: {iint: {}} for iint in range(1, 12) for fold in range(1, k_folds+1)}\n",
    "    cv_results = {}\n",
    "    for fold in range(1, 6):\n",
    "        cv_results[fold] = {iint:{} for iint in range(1,6)}\n",
    "    cv_results_preview = {'tpr':[],'fpr':[]}\n",
    "    intensidades = ['70', '60', '50', '40', '30']\n",
    "\n",
    "    # For each fold\n",
    "    for fold in range(1, k_folds+1):\n",
    "\n",
    "        # For every intensity\n",
    "        for iint in range(5, 6):\n",
    "            print(f'Intensidade: {intensidades[iint-1]} dB')\n",
    "            hist_tpr = []\n",
    "            hist_fpr = []\n",
    "\n",
    "            ### Take one random voluntary as fine-tuning instance, and 10 others as validation\n",
    "            train_index, test_index = train_test_split(np.arange(1,12), train_size=1)\n",
    "\n",
    "            # Fine-tune once\n",
    "            for ivol in train_index:\n",
    "                data = c_states_exp_filt[ivol, iint]\n",
    "                fine_tune_model(model_class, pretrained_model_path, save_model_path, data, \n",
    "                                reset_model = True # resets model at start of each intensity\n",
    "                                )\n",
    "\n",
    "            # Test in 10 other voluntaries\n",
    "            for ivol in test_index:\n",
    "                    data = c_states_exp_filt[ivol, iint]\n",
    "                    validate_model(model_class, save_model_path, data, hist_tpr, hist_fpr)\n",
    "\n",
    "            cv_results[fold][iint]['tpr'] = hist_tpr\n",
    "            cv_results[fold][iint]['fpr'] = hist_fpr\n",
    "\n",
    "        cv_results_preview['tpr'].append(np.mean(hist_tpr))\n",
    "        cv_results_preview['fpr'].append(np.mean(hist_fpr))\n",
    "\n",
    "    mtpr = np.mean(cv_results_preview['tpr'])\n",
    "    stpr = np.std(cv_results_preview['tpr']) \n",
    "    mfpr = np.mean(cv_results_preview['fpr'])\n",
    "    sfpr = np.std(cv_results_preview['fpr']) \n",
    "    print(f'Final TPR: { mtpr } +/- { stpr }')\n",
    "    print(f'Final FPR: { mfpr } +/- { sfpr }')\n",
    "\n",
    "    return cv_results, cv_results_preview\n",
    "\n",
    "# Fine-tune and validate models\n",
    "cv_dqn, cvp_dqn = cross_validate_model(DQN, './epd-kit/models/minimal_dqn_snr10.zip', './epd-kit/models/cv-exp-minimal_dqn_snr10.zip',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra resultados experimentais\n",
    "def get_fold_metrics(cv_dict,intensity, metric, prev = False):\n",
    "    collected_metric = []\n",
    "    for fold in cv_dict.keys():\n",
    "        if not(prev):\n",
    "            for intensity in np.arange(1,6):\n",
    "                for val in cv_dict[fold][intensity][metric]:\n",
    "                    collected_metric.append(val)\n",
    "        else:\n",
    "            for val in cv_dict[fold][1][metric]:\n",
    "                collected_metric.append(val)\n",
    "\n",
    "    return collected_metric\n",
    "\n",
    "inten = 4\n",
    "tpr = pd.DataFrame({'DQN':cvp_dqn['tpr']})\n",
    "\n",
    "fpr = pd.DataFrame({'DQN':cvp_dqn['fpr']})\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "sns.boxplot(tpr,gap=.8)\n",
    "plt.title('Detection Rate in Experimental ASSR')\n",
    "plt.ylabel('True Positive Rate [%]')\n",
    "plt.xlabel('DRL Algorithm')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.boxplot(fpr, gap=.8)\n",
    "plt.title('False Alarm Rate in Experimental ASSR')\n",
    "plt.ylabel('False Positive Rate [%]')\n",
    "plt.xlabel('DRL Algorithm')\n",
    "plt.ylim([0,30])\n",
    "plt.tight_layout()\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos completos\n",
    "\n",
    "Pré-treinados a 5, 2.5, 0, -2.5 e -5 dB SNR, com duas camadas ocultas tendo 64 neurônios com ativação ReLU.\n",
    "\n",
    "*(12.5k episódios de treino no total)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-treinamento\n",
    "\n",
    "![](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/epd-kit/imgs/convergence_demo_paper_alt.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste\n",
    "\n",
    "![](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/epd-kit/imgs/pd_snr_sc.png?raw=true)\n",
    "<br>\n",
    "![](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/epd-kit/imgs/fpr_snr_sc.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação cruzada\n",
    "\n",
    "![](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/epd-kit/imgs/cv-comp-3-models.png?raw=true)\n",
    "<!-- ![](https://github.com/Alexandre-Caldeira/drl-ord/blob/main/epd-kit/imgs/exp_results_alt.png?raw=true) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
